{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81739,"sourceType":"datasetVersion","datasetId":6776}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:45.243617Z","iopub.execute_input":"2025-04-20T07:33:45.244193Z","iopub.status.idle":"2025-04-20T07:33:45.504276Z","shell.execute_reply.started":"2025-04-20T07:33:45.244170Z","shell.execute_reply":"2025-04-20T07:33:45.503527Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/poetry/Kanye_West.txt\n/kaggle/input/poetry/johnny-cash.txt\n/kaggle/input/poetry/kanye-west.txt\n/kaggle/input/poetry/bruno-mars.txt\n/kaggle/input/poetry/dickinson.txt\n/kaggle/input/poetry/amy-winehouse.txt\n/kaggle/input/poetry/blink-182.txt\n/kaggle/input/poetry/paul-simon.txt\n/kaggle/input/poetry/patti-smith.txt\n/kaggle/input/poetry/bieber.txt\n/kaggle/input/poetry/disney.txt\n/kaggle/input/poetry/jimi-hendrix.txt\n/kaggle/input/poetry/lin-manuel-miranda.txt\n/kaggle/input/poetry/adele.txt\n/kaggle/input/poetry/dj-khaled.txt\n/kaggle/input/poetry/beatles.txt\n/kaggle/input/poetry/r-kelly.txt\n/kaggle/input/poetry/lady-gaga.txt\n/kaggle/input/poetry/radiohead.txt\n/kaggle/input/poetry/britney-spears.txt\n/kaggle/input/poetry/alicia-keys.txt\n/kaggle/input/poetry/rihanna.txt\n/kaggle/input/poetry/joni-mitchell.txt\n/kaggle/input/poetry/dolly-parton.txt\n/kaggle/input/poetry/drake.txt\n/kaggle/input/poetry/Lil_Wayne.txt\n/kaggle/input/poetry/notorious_big.txt\n/kaggle/input/poetry/eminem.txt\n/kaggle/input/poetry/janisjoplin.txt\n/kaggle/input/poetry/prince.txt\n/kaggle/input/poetry/bruce-springsteen.txt\n/kaggle/input/poetry/bob-dylan.txt\n/kaggle/input/poetry/notorious-big.txt\n/kaggle/input/poetry/lil-wayne.txt\n/kaggle/input/poetry/dr-seuss.txt\n/kaggle/input/poetry/nicki-minaj.txt\n/kaggle/input/poetry/bob-marley.txt\n/kaggle/input/poetry/al-green.txt\n/kaggle/input/poetry/nickelback.txt\n/kaggle/input/poetry/michael-jackson.txt\n/kaggle/input/poetry/lorde.txt\n/kaggle/input/poetry/kanye.txt\n/kaggle/input/poetry/leonard-cohen.txt\n/kaggle/input/poetry/ludacris.txt\n/kaggle/input/poetry/bjork.txt\n/kaggle/input/poetry/nursery_rhymes.txt\n/kaggle/input/poetry/nirvana.txt\n/kaggle/input/poetry/cake.txt\n/kaggle/input/poetry/missy-elliott.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import GPT2Tokenizer, TFGPT2LMHeadModel\nimport tensorflow as tf\ndata_files = {\n    \"train\": [\n\"/kaggle/input/poetry/bieber.txt\"    ]\n}\ndataset = load_dataset(\"text\", data_files=data_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:45.505469Z","iopub.execute_input":"2025-04-20T07:33:45.505691Z","iopub.status.idle":"2025-04-20T07:33:45.619157Z","shell.execute_reply.started":"2025-04-20T07:33:45.505673Z","shell.execute_reply":"2025-04-20T07:33:45.618461Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print((dataset[\"train\"][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:45.619959Z","iopub.execute_input":"2025-04-20T07:33:45.620279Z","iopub.status.idle":"2025-04-20T07:33:45.624256Z","shell.execute_reply.started":"2025-04-20T07:33:45.620260Z","shell.execute_reply":"2025-04-20T07:33:45.623606Z"}},"outputs":[{"name":"stdout","text":"{'text': 'What do you mean?'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:45.625688Z","iopub.execute_input":"2025-04-20T07:33:45.625877Z","iopub.status.idle":"2025-04-20T07:33:47.745269Z","shell.execute_reply.started":"2025-04-20T07:33:45.625862Z","shell.execute_reply":"2025-04-20T07:33:47.744737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20dc06ec5e9d42a980aecfc4e64e427c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e07105a1805041b9a0271de73a89b31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17447062ad8f4f0bac003521a70c5746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54eabf63505640219642e91a26ce2928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f47ede31ba44a3b44ede7dc8862cd9"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def tokenize_function(txt):\n    return tokenizer(txt[\"text\"], truncation=True, padding=\"max_length\", max_length=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:47.746005Z","iopub.execute_input":"2025-04-20T07:33:47.746267Z","iopub.status.idle":"2025-04-20T07:33:47.749954Z","shell.execute_reply.started":"2025-04-20T07:33:47.746242Z","shell.execute_reply":"2025-04-20T07:33:47.749202Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def gen():\n    for row in tokenized_dataset[\"train\"]:\n        yield (\n            {\"input_ids\": row[\"input_ids\"], \"attention_mask\": row[\"attention_mask\"]},\n            row[\"input_ids\"]\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:47.750864Z","iopub.execute_input":"2025-04-20T07:33:47.751143Z","iopub.status.idle":"2025-04-20T07:33:47.768007Z","shell.execute_reply.started":"2025-04-20T07:33:47.751120Z","shell.execute_reply":"2025-04-20T07:33:47.767329Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenized_dataset = dataset.map(tokenize_function, batched=True)\ntf_dataset = tf.data.Dataset.from_generator(\n    gen,\n    output_signature=(\n        {\n            \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n            \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n        },\n        tf.TensorSpec(shape=(128,), dtype=tf.int32),\n    )\n).shuffle(1000).batch(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:47.768783Z","iopub.execute_input":"2025-04-20T07:33:47.769257Z","iopub.status.idle":"2025-04-20T07:33:49.633588Z","shell.execute_reply.started":"2025-04-20T07:33:47.769231Z","shell.execute_reply":"2025-04-20T07:33:49.632776Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3715 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f6df1779ca461ab6bb6ff39f3c7911"}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1745134429.547343      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745134429.548105      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:49.634723Z","iopub.execute_input":"2025-04-20T07:33:49.634987Z","iopub.status.idle":"2025-04-20T07:33:54.002631Z","shell.execute_reply.started":"2025-04-20T07:33:49.634965Z","shell.execute_reply":"2025-04-20T07:33:54.002065Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932b701708da41bb9fedfef3e188ace2"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n\nAll the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model.fit(tf_dataset, epochs=5, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:33:54.003328Z","iopub.execute_input":"2025-04-20T07:33:54.003553Z","iopub.status.idle":"2025-04-20T07:49:37.614042Z","shell.execute_reply.started":"2025-04-20T07:33:54.003537Z","shell.execute_reply":"2025-04-20T07:49:37.613261Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745134468.941898     102 service.cc:148] XLA service 0x7a78bc3038b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745134468.942996     102 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745134468.943015     102 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745134469.034689     102 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745134469.158929     102 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"929/929 [==============================] - 217s 186ms/step - loss: 0.2932\nEpoch 2/5\n929/929 [==============================] - 182s 195ms/step - loss: 0.1959\nEpoch 3/5\n929/929 [==============================] - 182s 195ms/step - loss: 0.1512\nEpoch 4/5\n929/929 [==============================] - 182s 195ms/step - loss: 0.1180\nEpoch 5/5\n929/929 [==============================] - 182s 195ms/step - loss: 0.0950\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7a7a5fce9290>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model.save_pretrained(\"gpt2-poetry-tf\")\ntokenizer.save_pretrained(\"gpt2-poetry-tf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:49:37.615972Z","iopub.execute_input":"2025-04-20T07:49:37.616167Z","iopub.status.idle":"2025-04-20T07:49:39.087794Z","shell.execute_reply.started":"2025-04-20T07:49:37.616152Z","shell.execute_reply":"2025-04-20T07:49:39.087164Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('gpt2-poetry-tf/tokenizer_config.json',\n 'gpt2-poetry-tf/special_tokens_map.json',\n 'gpt2-poetry-tf/vocab.json',\n 'gpt2-poetry-tf/merges.txt',\n 'gpt2-poetry-tf/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\nmodel = TFGPT2LMHeadModel.from_pretrained(\"gpt2-poetry-tf\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-poetry-tf\")\nprompt = \"First you wanna go\"\ninput_ids = tokenizer.encode(prompt, return_tensors='tf')\n\noutput = model.generate(\n    input_ids,\n    max_new_tokens=80,\n    temperature=1.0,\n    top_k=500,\n    top_p=0.95,\n    do_sample=True,\n    no_repeat_ngram_size=3,\n    pad_token_id=tokenizer.eos_token_id,\n)\n\ngenerated_poem = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(generated_poem)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:49:39.088523Z","iopub.execute_input":"2025-04-20T07:49:39.088742Z","iopub.status.idle":"2025-04-20T07:49:44.247384Z","shell.execute_reply.started":"2025-04-20T07:49:39.088725Z","shell.execute_reply":"2025-04-20T07:49:44.246685Z"}},"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n\nAll the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-poetry-tf.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"First you wanna go to the left and you want to turn right (ooh)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}